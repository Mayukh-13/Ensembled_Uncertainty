{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e58d060-0c47-4feb-afca-9fb888b1e842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.utils as vision_utils\n",
    "import copy\n",
    "import json\n",
    "import math\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "from models import get_model_func\n",
    "from utils import get_acc_ensemble, get_acc\n",
    "from utils import dl_to_sampler\n",
    "from data import get_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e30c92b-3417-46f1-a93b-097c1a688435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install wilds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8fff380-ac8d-42cd-9fae-198f04e09645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # General training params\n",
    "    parser.add_argument('--ensemble_size', default=2, type=int)\n",
    "    parser.add_argument('--batch_size_train', default=256, type=int)\n",
    "    parser.add_argument('--batch_size_eval', default=512, type=int)\n",
    "    parser.add_argument('--seed', default=0, type=int)\n",
    "    parser.add_argument('--device', default='cuda:0', type=str)\n",
    "    parser.add_argument('--epochs', default=50, type=int)\n",
    "    parser.add_argument('--lr', default=0.001, type=float)\n",
    "    parser.add_argument('--l2_reg', default=0.0005, type=float)\n",
    "    parser.add_argument('--scheduler', default='none', choices=['triangle', 'multistep', 'cosine', 'none'])\n",
    "    parser.add_argument('--opt', default='adam', choices=['adamw', 'sgd'])\n",
    "    parser.add_argument('--eval_freq', default=50, type=int) # in iterations\n",
    "    parser.add_argument('--ckpt_freq', default=1, type=int) # in epochs\n",
    "    parser.add_argument('--results_base_folder', default=\"./exps\", type=str) # in epochs\n",
    "    # Diversity params\n",
    "    parser.add_argument('--no_diversity', action='store_true')\n",
    "    parser.add_argument('--dbat_loss_type', default='v1', choices=['v1', 'v2'])\n",
    "    parser.add_argument('--perturb_type', default='ood_is_test', choices=['ood_is_test', 'ood_is_not_test'])\n",
    "    parser.add_argument('--alpha', default=1.0, type=float)    \n",
    "    # Dataset and model\n",
    "    parser.add_argument('--model', default='resnet50', choices=['resnet18', 'resnet50'])\n",
    "    parser.add_argument('--dataset', default='camelyon17', choices=['waterbird', 'camelyon17', 'oh-65cls'])\n",
    "    return parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4903876-a899-4e85-aeae-f5b24dac20ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(get_model, get_opt, num_models, train_dl, valid_dl, test_dl, perturb_dl, get_scheduler=None, max_epoch=10, \n",
    "          eval_freq=400, ckpt_freq=1, ckpt_path=\"\", alpha=1.0, use_diversity_reg=True, dbat_loss_type='v1', extra_args=None):\n",
    "    \n",
    "    ensemble = [get_model() for _ in range(num_models)]\n",
    "    ensemble_early_stopped = [None for _ in range(num_models)]\n",
    "    \n",
    "    last_opt = None\n",
    "    last_scheduler = None\n",
    "    start_epoch = 0\n",
    "    start_m_idx = 0\n",
    "    last_best_valid_acc = -1\n",
    "    itr = -1\n",
    "\n",
    "    stats = {f\"m{i+1}\": {\"valid-acc\": [], \"erm-loss\": [], \"adv-loss\": []} for i in range(len(ensemble))}\n",
    "    stats['ensemble-test-acc'] = None\n",
    "    stats['ensemble-test-pgd-acc'] = None\n",
    "    stats['ensemble-test-acc-es'] = None\n",
    "    stats['ensemble-test-pgd-acc-es'] = None\n",
    "\n",
    "    for model in ensemble:\n",
    "        model.train()\n",
    "        \n",
    "    for m_idx in range(start_m_idx, num_models):\n",
    "        m = ensemble[m_idx]\n",
    "        \n",
    "        for m_ in ensemble[:m_idx]:\n",
    "            m_.eval()\n",
    "        \n",
    "        opt = get_opt(m.parameters())\n",
    "        scheduler = get_scheduler(opt) if get_scheduler is not None else None\n",
    "        \n",
    "        perturb_sampler = dl_to_sampler(perturb_dl)\n",
    "        \n",
    "        for epoch in range(start_epoch, max_epoch):\n",
    "            for x, y in train_dl:\n",
    "                itr += 1\n",
    "                \n",
    "                x_tilde = perturb_sampler()[0]\n",
    "                \n",
    "                erm_loss = F.cross_entropy(m(x), y)\n",
    "                \n",
    "                if use_diversity_reg and m_idx != 0:\n",
    "                    \n",
    "                    if dbat_loss_type == 'v1':\n",
    "                        adv_loss = []\n",
    "\n",
    "                        p_1_s, indices = [], []\n",
    "                        with torch.no_grad():\n",
    "                            for m_ in ensemble[:m_idx]:\n",
    "                                p_1 = torch.softmax(m_(x_tilde), dim=1)\n",
    "                                p_1, idx = p_1.max(dim=1)\n",
    "                                p_1_s.append(p_1)\n",
    "                                indices.append(idx)\n",
    "\n",
    "                        p_2 = torch.softmax(m(x_tilde), dim=1)\n",
    "                        p_2_s = [p_2[torch.arange(len(p_2)), max_idx] for max_idx in indices]\n",
    "\n",
    "                        for i in range(len(p_1_s)):\n",
    "                            al = (- torch.log(p_1_s[i] * (1-p_2_s[i]) + p_2_s[i] * (1-p_1_s[i]) +  1e-7)).mean()\n",
    "                            adv_loss.append(al)\n",
    "                            \n",
    "                    elif dbat_loss_type == 'v2':\n",
    "                        adv_loss = []\n",
    "                        p_2 = torch.softmax(m(x_tilde), dim=1)\n",
    "                        p_2_1, max_idx = p_2.max(dim=1) # proba of class 1 for m\n",
    "                        \n",
    "                        with torch.no_grad():\n",
    "                            p_1_s = [torch.softmax(m_(x_tilde), dim=1) for m_ in ensemble[:m_idx]]\n",
    "                            p_1_1_s = [p_1[torch.arange(len(p_1)), max_idx] for p_1 in p_1_s] # probas of class 1 for m_\n",
    "                            \n",
    "                        for i in range(len(p_1_s)):\n",
    "                            al = (- torch.log(p_1_1_s[i] * (1.0 - p_2_1) + p_2_1 * (1.0 - p_1_1_s[i]) +  1e-7)).mean()\n",
    "                            adv_loss.append(al)\n",
    "                        \n",
    "                    else:\n",
    "                        raise NotImplementedError(f\"Unknown adversarial loss type: '{dbat_loss_type}'\")\n",
    "                else:\n",
    "                    adv_loss = [torch.tensor([0]).to(x.device)]\n",
    "\n",
    "                adv_loss = sum(adv_loss)/len(adv_loss)\n",
    "                loss = erm_loss + alpha * adv_loss\n",
    "\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "\n",
    "                if itr % eval_freq == 0:\n",
    "                    m.eval()\n",
    "                    valid_acc = get_acc(m, valid_dl)\n",
    "                    p_s = f\"[m{m_idx+1}] {epoch}:{itr} [train] erm-loss: {erm_loss.item():.3f},\"  + \\\n",
    "                          f\" adv-loss: {adv_loss.item():.3f} [valid] acc: {valid_acc:.3f} \"\n",
    "                    stats[f\"m{m_idx+1}\"][\"valid-acc\"].append((itr, valid_acc))\n",
    "                    stats[f\"m{m_idx+1}\"][\"erm-loss\"].append((itr, erm_loss.item()))\n",
    "                    stats[f\"m{m_idx+1}\"][\"adv-loss\"].append((itr, adv_loss.item()))\n",
    "                    if valid_acc > last_best_valid_acc:\n",
    "                        last_best_valid_acc = valid_acc\n",
    "                        ensemble_early_stopped[m_idx] = copy.deepcopy(m.state_dict())\n",
    "                    if itr != 0 and scheduler is not None:\n",
    "                        p_s += f\"[lr] {scheduler.get_last_lr()[0]:.5f} \"\n",
    "                    print(p_s)\n",
    "                    if math.isnan(loss.item()): \n",
    "                        raise(ValueError(\"Loss is NaN. :(\"))\n",
    "                    m.train()\n",
    "                \n",
    "            if epoch % ckpt_freq == 0:\n",
    "                torch.save({'ensemble': [model.state_dict() for model in ensemble], \n",
    "                            'ensemble_early_stopped': ensemble_early_stopped, \n",
    "                            'last_opt': opt.state_dict(),\n",
    "                            'last_scheduler': scheduler.state_dict() if scheduler is not None else None,\n",
    "                            'last_epoch': epoch,\n",
    "                            'last_m_idx': m_idx,\n",
    "                            'last_itr': itr,\n",
    "                            'last_best_valid_acc': last_best_valid_acc,\n",
    "                           }, ckpt_path)   \n",
    "        \n",
    "        itr = -1\n",
    "        last_best_valid_acc = -1\n",
    "        \n",
    "    stats['test-acc'] = []\n",
    "    for i, model in enumerate(ensemble): # test acc for each predictor in ensemble\n",
    "        model.eval()  \n",
    "        test_acc = get_acc(model, test_dl)\n",
    "        stats['test-acc'].append(test_acc)\n",
    "        print(f\"[test m{i+1}] test-acc: {test_acc:.3f}\")\n",
    "        \n",
    "    test_acc_ensemble = get_acc_ensemble(ensemble, test_dl)\n",
    "    stats['ensemble-test-acc'] = test_acc_ensemble\n",
    "    print(f\"[test (last iterates ensemble)] test-acc: {test_acc_ensemble:.3f}\") \n",
    "    \n",
    "    test_acc_ensemble_per_ens_size = None\n",
    "    if len(ensemble) > 2: # ensemble test accs for sub-ensembles\n",
    "        test_acc_ensemble_per_ens_size = [get_acc_ensemble(ensemble[:ne], test_dl) for ne in range(2, len(ensemble)+1)]\n",
    "        ens_gs = \", \".join([f\"{x:.3f}\" for x in test_acc_ensemble_per_ens_size])\n",
    "        print(f\"[test ensemble given size] {stats['test-acc'][0]:.3f}, {ens_gs}\")\n",
    "    stats['test_acc_ensemble_per_ens_size'] = test_acc_ensemble_per_ens_size\n",
    "\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92a0e707-9b86-42c3-b9fd-43da0d2cad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args): \n",
    "    \n",
    "    args.device = torch.device(args.device)\n",
    "    \n",
    "    torch.manual_seed(args.seed)\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    \n",
    "    print(f\"Loading dataset '{args.dataset}'\")\n",
    "    \n",
    "    train_dl, valid_dl, test_dl, perturb_dl = get_dataset(args)\n",
    "        \n",
    "    print(f\"Train dataset length: {len(train_dl.dataset)}\")\n",
    "    print(f\"Valid dataset length: {len(valid_dl.dataset)}\")\n",
    "    print(f\"Test dataset length: {len(test_dl.dataset)}\")\n",
    "    print(f\"Perturbations dataset length: {len(perturb_dl.dataset)}\")\n",
    "    \n",
    "    get_model = get_model_func(args)\n",
    "    \n",
    "    if args.opt == 'adamw':\n",
    "        get_opt = lambda p: torch.optim.AdamW(p, lr=args.lr, weight_decay=0.05)\n",
    "    else:\n",
    "        get_opt = lambda p: torch.optim.SGD(p, lr=args.lr, momentum=0.9, weight_decay=args.l2_reg)#, nesterov=True)\n",
    "    \n",
    "    if args.scheduler != 'none':\n",
    "        if args.scheduler == 'triangle':\n",
    "            get_scheduler = lambda opt: torch.optim.lr_scheduler.CyclicLR(opt, 0, args.lr, \n",
    "                                                                          step_size_up=(len(train_dl)*args.epochs)//2, \n",
    "                                                                          mode='triangular', cycle_momentum=False)\n",
    "        elif args.scheduler == 'cosine':\n",
    "            get_scheduler = lambda opt: torch.optim.lr_scheduler.CyclicLR(opt, 0, args.lr, \n",
    "                                                                          step_size_up=(len(train_dl)*args.epochs)//2, \n",
    "                                                                          mode='cosine', cycle_momentum=False)\n",
    "        elif args.scheduler == 'multistep':\n",
    "            n_iters = len(train_dl)*args.epochs\n",
    "            milestones = [0.25*n_iters, 0.5*n_iters, 0.75*n_iters] # hard-coded steps for now, suitable for resnet18\n",
    "            get_scheduler = lambda opt: torch.optim.lr_scheduler.MultiStepLR(opt, milestones=milestones, gamma=0.3)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Unknown scheduler type: {args.scheduler}.\")\n",
    "    else:\n",
    "        get_scheduler = None\n",
    "\n",
    "        \n",
    "    exp_name = f\"ep={args.epochs}_lrmax={args.lr}_alpha={args.alpha}_dataset={args.dataset}_perturb_type={args.perturb_type}\" + \\\n",
    "               f\"_model={args.model}_scheduler={args.scheduler}_seed={args.seed}_opt={args.opt}_ensemble_size={args.ensemble_size}\" + \\\n",
    "               f\"_no_diversity={args.no_diversity}_dbat_loss_type={args.dbat_loss_type}_weight_decay={args.l2_reg}_no_nesterov_\"\n",
    "    \n",
    "    ckpt_path = f\"{args.results_base_folder}/{args.dataset}/perturb={args.perturb_type}/{args.model}/ep{args.epochs}/{exp_name}\"\n",
    "    if not os.path.exists(ckpt_path):\n",
    "        os.makedirs(ckpt_path)\n",
    "    else:\n",
    "        if os.path.isfile(f\"{ckpt_path}/summary.json\"): # the experiment was already completed\n",
    "            sys.exit(0)\n",
    "            \n",
    "    print(f\"\\nTraining \\n{vars(args)}\\n\")\n",
    "    stats = train(get_model, get_opt, args.ensemble_size, train_dl, valid_dl, test_dl, perturb_dl, get_scheduler, args.epochs, \n",
    "                  eval_freq=args.eval_freq, ckpt_freq=1, ckpt_path=f\"{ckpt_path}/ckpt.pt\", alpha=args.alpha, \n",
    "                  use_diversity_reg=not args.no_diversity, dbat_loss_type=args.dbat_loss_type, extra_args=args)\n",
    "    \n",
    "    args.device = None\n",
    "    stats['args'] = vars(args)\n",
    "    \n",
    "    with open(f\"{ckpt_path}/summary.json\", \"w\") as fs:\n",
    "        json.dump(stats, fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5b5f029-535b-4bda-af83-a204713ace24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--ensemble_size ENSEMBLE_SIZE] [--batch_size_train BATCH_SIZE_TRAIN]\n",
      "                             [--batch_size_eval BATCH_SIZE_EVAL] [--seed SEED] [--device DEVICE] [--epochs EPOCHS]\n",
      "                             [--lr LR] [--l2_reg L2_REG] [--scheduler {triangle,multistep,cosine,none}]\n",
      "                             [--opt {adamw,sgd}] [--eval_freq EVAL_FREQ] [--ckpt_freq CKPT_FREQ]\n",
      "                             [--results_base_folder RESULTS_BASE_FOLDER] [--no_diversity] [--dbat_loss_type {v1,v2}]\n",
      "                             [--perturb_type {ood_is_test,ood_is_not_test}] [--alpha ALPHA]\n",
      "                             [--model {resnet18,resnet50}] [--dataset {waterbird,camelyon17,oh-65cls}]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\baner\\AppData\\Roaming\\jupyter\\runtime\\kernel-4a301b62-b665-4b80-9e4a-e7d51c2600a1.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    args = get_args()\n",
    "    \n",
    "    main(args)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc6528f8-c49d-4334-9e76-b87cc9d788f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 3\u001b[0m     args \u001b[38;5;241m=\u001b[39m \u001b[43mget_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     main(args)\n",
      "Cell \u001b[1;32mIn[4], line 25\u001b[0m, in \u001b[0;36mget_args\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--model\u001b[39m\u001b[38;5;124m'\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresnet50\u001b[39m\u001b[38;5;124m'\u001b[39m, choices\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresnet18\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresnet50\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     24\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--dataset\u001b[39m\u001b[38;5;124m'\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcamelyon17\u001b[39m\u001b[38;5;124m'\u001b[39m, choices\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwaterbird\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcamelyon17\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moh-65cls\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\argparse.py:1894\u001b[0m, in \u001b[0;36mArgumentParser.parse_args\u001b[1;34m(self, args, namespace)\u001b[0m\n\u001b[0;32m   1892\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m argv:\n\u001b[0;32m   1893\u001b[0m     msg \u001b[38;5;241m=\u001b[39m _(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munrecognized arguments: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1894\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43margv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1895\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\argparse.py:2655\u001b[0m, in \u001b[0;36mArgumentParser.error\u001b[1;34m(self, message)\u001b[0m\n\u001b[0;32m   2653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_usage(_sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m   2654\u001b[0m args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprog\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprog, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m: message}\n\u001b[1;32m-> 2655\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m%(prog)s\u001b[39;49;00m\u001b[38;5;124;43m: error: \u001b[39;49m\u001b[38;5;132;43;01m%(message)s\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\argparse.py:2642\u001b[0m, in \u001b[0;36mArgumentParser.exit\u001b[1;34m(self, status, message)\u001b[0m\n\u001b[0;32m   2640\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message:\n\u001b[0;32m   2641\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_message(message, _sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m-> 2642\u001b[0m \u001b[43m_sys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mSystemExit\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "%tb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432a2509-ed18-4b82-8daf-cb71a8970006",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
